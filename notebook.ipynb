{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 7., 9.]])\n",
      "tensor([[ 6.],\n",
      "        [15.]])\n",
      "tensor([[0.2006, 0.1992, 0.2005, 0.1981, 0.2015],\n",
      "        [0.1987, 0.2004, 0.1999, 0.1989, 0.2022]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "#softmax回归实现\n",
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256 \n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs),requires_grad=True)\n",
    "b = torch.zeros(num_outputs,requires_grad=True)\n",
    "# 定义softmax操作\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    par = X_exp.sum(1,keepdim=True)\n",
    "    return X_exp / par\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]])\n",
    "print(x.sum(axis = 0,keepdim=True))\n",
    "print(x.sum(axis = 1,keepdim=True))\n",
    "\n",
    "X = torch.normal(0, 0.01,(2, 5))\n",
    "X_cal = softmax(X)\n",
    "print(X_cal) \n",
    "print(X_cal.sum(1, keepdim=True))\n",
    "\n",
    "# 定义模型\n",
    "def net(X):\n",
    "    return softmax(torch.matual(X.reshape((-1, W.shape[0])), W) + b)\n",
    "\n",
    "#定义损失函数\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)),y])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[-0.6874, -1.3592,  0.3876],\n",
      "        [ 1.1140,  0.9391, -0.6689]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "cpu\n",
      "tensor([[ 0.7606,  1.0468, -2.3401],\n",
      "        [-0.7889, -0.2347, -0.1253]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 2x3 的全 0 张量\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "\n",
    "# 创建一个 2x3 的全 1 张量\n",
    "b = torch.ones(2, 3)\n",
    "print(b)\n",
    "\n",
    "# 创建一个 2x3 的随机数张量\n",
    "c = torch.randn(2, 3)\n",
    "print(c)\n",
    "\n",
    "# 从 NumPy 数组创建张量\n",
    "import numpy as np\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "# 在指定设备（CPU/GPU）上创建张量\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "d = torch.randn(2, 3, device=device)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移动到GPU进行运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将张量移动到GPU进行加速\n",
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor_from_list.to('cuda')\n",
    "    print(tensor_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度和自动微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0149,  1.4972, -0.6675],\n",
      "        [ 4.9121,  3.0807,  1.2811]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "tensor_requires_grad = torch.randn(2, 3, requires_grad=True)\n",
    "tensor_result = tensor_requires_grad * 2\n",
    "print(tensor_result)\n",
    "# 计算张量的梯度，记得要求和\n",
    "tensor_result.sum().backward()\n",
    "print(tensor_requires_grad.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化内存管理方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.clone()`\n",
    "\n",
    "`.detach()`\n",
    "\n",
    "`.to()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.], requires_grad=True)\n",
      "tensor([-1.,  2.], grad_fn=<CopySlices>)\n",
      "None\n",
      "tensor([0., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21260\\899562095.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "#.clone() 方法会创建一个张量的副本，但不会保留梯度信息\n",
    "x = torch.tensor([1.0,2],requires_grad=True)\n",
    "y = x.clone()\n",
    "y[0] = -1\n",
    "print(x)\n",
    "print(y)\n",
    "y.sum().backward()\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.detach()` 用于从计算图中分离张量，生成一个不需要梯度计算的副本。分离后的张量不再参与反向传播，这在以下情况中非常有用：\n",
    "冻结网络参数：在迁移学习中，你可能希望冻结某些层的参数，阻止它们更新梯度。\n",
    "减少内存开销：通过移除不需要梯度的张量，可以减小计算图的规模，从而节省内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach()\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "# 分离意味着b不会跟踪任何操作历史\n",
    "b = a.detach()\n",
    "# requires_grad的属性会被设置为False\n",
    "print(b.requires_grad)  # 输出: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络nn.Mudule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的全连接神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 输入 x (1×2) 与权重矩阵 W₁ (2×2) 相乘，然后加上偏置 b₁ (2)，结果形状: (1, 2)\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        # 隐藏层输出 (1×2) 与权重矩阵 W₂ (2×1) 相乘，然后加上偏置 b₂ (1)，结果形状: (1, 1)\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        # ReLu激活函数定义f(x) = max(0, x)，正区间的梯度恒为1\n",
    "        #这里将Relu激活函数作用在全连接层的输出上，它激活了部分特征（正值），抑制了其他特征（负值）\n",
    "        x = self.fc2(x)\n",
    "        # 经过ReLu的处理后的数据再传入第二层全连接层，得到最终的输出\n",
    "        return x\n",
    "\n",
    "# 创建网络实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播（Forward Propagation）： \n",
    "\n",
    "在前向传播阶段，输入数据通过网络层传递，每层应用权重和激活函数，直到产生输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算损失（Calculate Loss）： \n",
    "\n",
    "根据网络的输出和真实标签，计算损失函数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6315, 0.0088]])\n",
      "tensor([[0.7926]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1386]])\n",
      "tensor(0.8672, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 随机输入\n",
    "x = torch.randn(1, 2)\n",
    "print(x)\n",
    "# 前向传播\n",
    "output = model(x) # model在前面定义神经网络中出现过\n",
    "print(output)\n",
    "\n",
    "# 定义损失函数（例如均方误差 MSE）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 假设目标值为 1\n",
    "target = torch.randn(1, 1)\n",
    "print(target)\n",
    "\n",
    "# 计算损失\n",
    "# output：模型的预测值；target：期望的目标值，二者都是(1,1)的张量\n",
    "loss = criterion(output, target) #计算均方误差\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器(Optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "优化器在训练过程中更新神经网络的参数，以减少损失函数的值。\n",
    "\n",
    "PyTorch 提供了多种优化器，例如 SGD、Adam 等。\n",
    "\n",
    "使用优化器进行参数更新："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.Size([2, 2])\n",
      "fc1.bias: torch.Size([2])\n",
      "fc2.weight: torch.Size([1, 2])\n",
      "fc2.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 查看model里所有的模型参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# 输出类似：\n",
    "# fc1.weight: torch.Size([2, 2])\n",
    "# fc1.bias: torch.Size([2])\n",
    "# fc2.weight: torch.Size([1, 2])\n",
    "# fc2.bias: torch.Size([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器（使用 Adam 优化器）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 当调用 parameters() 方法时，它会递归地收集所有这些参数\n",
    "\n",
    "# 训练步骤\n",
    "optimizer.zero_grad()  # 清空梯度\n",
    "loss.backward()  # 反向传播\n",
    "# 每次调用 optimizer.step() 时，会使用计算出的梯度更新这些参数\n",
    "optimizer.step()  # 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型通常包括以下几个步骤：  \n",
    "\n",
    "#### 数据准备：  \n",
    "\n",
    "- 收集和处理数据，包括清洗、标准化和归一化。  \n",
    "- 将数据分为训练集、验证集和测试集。  \n",
    "\n",
    "#### 定义模型：  \n",
    "\n",
    "- 选择模型架构，例如决策树、神经网络等。  \n",
    "- 初始化模型参数（权重和偏置）。  \n",
    "\n",
    "#### 选择损失函数：  \n",
    "\n",
    "- 根据任务类型（如分类、回归）选择合适的损失函数。  \n",
    "\n",
    "#### 选择优化器：  \n",
    "\n",
    "- 选择一个优化算法，如SGD、Adam等，来更新模型参数。  \n",
    "\n",
    "#### 前向传播：  \n",
    "\n",
    "- 在每次迭代中，将输入数据通过模型传递，计算预测输出。  \n",
    "\n",
    "#### 计算损失：  \n",
    "\n",
    "- 使用损失函数评估预测输出与真实标签之间的差异。  \n",
    "\n",
    "#### 反向传播：  \n",
    "\n",
    "- 利用自动求导计算损失相对于模型参数的梯度。  \n",
    "\n",
    "#### 参数更新：  \n",
    "\n",
    "- 根据计算出的梯度和优化器的策略更新模型参数。  \n",
    "\n",
    "#### 迭代优化：  \n",
    "\n",
    "- 重复步骤5-8，直到模型在验证集上的性能不再提升或达到预定的迭代次数。  \n",
    "\n",
    "#### 评估和测试：  \n",
    "\n",
    "- 使用测试集评估模型的最终性能，确保模型没有过拟合。  \n",
    "\n",
    "#### 模型调优：  \n",
    "\n",
    "- 根据模型在测试集上的表现进行调参，如改变学习率、增加正则化等。  \n",
    "\n",
    "#### 部署模型：  \n",
    "\n",
    "- 将训练好的模型部署到生产环境中，用于实际的预测任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.4844\n",
      "Epoch [20/100], Loss: 1.4643\n",
      "Epoch [30/100], Loss: 1.4461\n",
      "Epoch [40/100], Loss: 1.4306\n",
      "Epoch [50/100], Loss: 1.4166\n",
      "Epoch [60/100], Loss: 1.4040\n",
      "Epoch [70/100], Loss: 1.3924\n",
      "Epoch [80/100], Loss: 1.3818\n",
      "Epoch [90/100], Loss: 1.3720\n",
      "Epoch [100/100], Loss: 1.3629\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 定义一个简单的神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. 创建模型实例\n",
    "model = SimpleNN()\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器\n",
    "\n",
    "# 4. 假设我们有训练数据 X 和 Y\n",
    "X = torch.randn(10, 2)  # 10 个样本，2 个特征\n",
    "Y = torch.randn(10, 1)  # 10 个目标值\n",
    "\n",
    "# 5. 训练循环\n",
    "for epoch in range(100):  # 训练 100 轮\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    output = model(X)  # 前向传播\n",
    "    loss = criterion(output, Y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    \n",
    "    # 每 10 轮输出一次损失\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
